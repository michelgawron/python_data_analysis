{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michelgawron/python_data_analysis/blob/master/data_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wHYO4eASie7W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import export_graphviz\n",
        "from IPython.display import Image\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nR5aOcD_ijPs",
        "colab_type": "code",
        "outputId": "13e3c694-bea3-41db-cb9a-ce494b085348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "# Downloading the files\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00459/avila.zip\n",
        "!unzip avila.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-15 13:00:59--  https://archive.ics.uci.edu/ml/machine-learning-databases/00459/avila.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 584600 (571K) [application/zip]\n",
            "Saving to: ‘avila.zip’\n",
            "\n",
            "avila.zip           100%[===================>] 570.90K  1.62MB/s    in 0.3s    \n",
            "\n",
            "2019-02-15 13:01:00 (1.62 MB/s) - ‘avila.zip’ saved [584600/584600]\n",
            "\n",
            "Archive:  avila.zip\n",
            "   creating: avila/\n",
            "  inflating: avila/avila-tr.txt      \n",
            "  inflating: avila/avila-ts.txt      \n",
            "  inflating: avila/avila-description.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g56GBpXRi-gn",
        "colab_type": "code",
        "outputId": "fd065c4a-1b2f-45ee-d6cc-b1bdf2c8977b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "cell_type": "code",
      "source": [
        "# First of all we need to read the data from txt files - the read_csv functions allows it even though files are not in csv format\n",
        "train_data = pd.read_csv('./avila/avila-tr.txt', sep=',', header=None)\n",
        "test_data = pd.read_csv('./avila/avila-ts.txt', sep=',', header=None)\n",
        "train_data.head(5), test_data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(         0         1         2         3         4         5         6   \\\n",
              " 0  0.266074 -0.165620  0.320980  0.483299  0.172340  0.273364  0.371178   \n",
              " 1  0.130292  0.870736 -3.210528  0.062493  0.261718  1.436060  1.465940   \n",
              " 2 -0.116585  0.069915  0.068476 -0.783147  0.261718  0.439463 -0.081827   \n",
              " 3  0.031541  0.297600 -3.210528 -0.583590 -0.721442 -0.307984  0.710932   \n",
              " 4  0.229043  0.807926 -0.052442  0.082634  0.261718  0.148790  0.635431   \n",
              " \n",
              "          7         8         9  10  \n",
              " 0  0.929823  0.251173  0.159345  A  \n",
              " 1  0.636203  0.282354  0.515587  A  \n",
              " 2 -0.888236 -0.123005  0.582939  A  \n",
              " 3  1.051693  0.594169 -0.533994  A  \n",
              " 4  0.051062  0.032902 -0.086652  F  ,\n",
              "          0         1         2         3         4         5         6   \\\n",
              " 0 -3.498799  0.250492  0.232070  1.224178 -4.922215  1.145386  0.182426   \n",
              " 1  0.204355 -0.354049  0.320980  0.410166 -0.989576 -2.218127  0.220177   \n",
              " 2  0.759828 -1.304042 -0.023991 -0.973663 -0.006417 -0.349509 -0.421580   \n",
              " 3 -0.005490  0.360409  0.281860 -0.213479 -1.168333 -1.013906 -0.346080   \n",
              " 4  0.080916  0.101320  0.104040  0.140490  0.261718  0.480988  0.710932   \n",
              " \n",
              "          7         8         9  10  \n",
              " 0 -0.165983 -0.123005  1.087144  W  \n",
              " 1  0.181844  2.090879 -2.009758  A  \n",
              " 2 -0.450127  0.469443  0.060952  I  \n",
              " 3  1.176165  0.968347 -0.627999  E  \n",
              " 4 -0.253430 -0.497183  0.155681  A  )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_5fJBRv2sJG6",
        "outputId": "c77d5699-537c-4e29-f22f-4033d4d29911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's rename columns for clarity\n",
        "columns = [\"intercolumnar distance\",\n",
        "\"upper margin\", \n",
        "\"lower margin\", \n",
        "\"exploitation\", \n",
        "\"row number\", \n",
        "\"modular ratio\", \n",
        "\"interlinear spacing\", \n",
        "\"weight\", \n",
        "\"peak number\", \n",
        "\"m_ratio/ i_spacing\",\n",
        "\"class\"]\n",
        "train_data.columns = columns\n",
        "test_data.columns = columns\n",
        "train_data.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intercolumnar distance</th>\n",
              "      <th>upper margin</th>\n",
              "      <th>lower margin</th>\n",
              "      <th>exploitation</th>\n",
              "      <th>row number</th>\n",
              "      <th>modular ratio</th>\n",
              "      <th>interlinear spacing</th>\n",
              "      <th>weight</th>\n",
              "      <th>peak number</th>\n",
              "      <th>m_ratio/ i_spacing</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.266074</td>\n",
              "      <td>-0.165620</td>\n",
              "      <td>0.320980</td>\n",
              "      <td>0.483299</td>\n",
              "      <td>0.172340</td>\n",
              "      <td>0.273364</td>\n",
              "      <td>0.371178</td>\n",
              "      <td>0.929823</td>\n",
              "      <td>0.251173</td>\n",
              "      <td>0.159345</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.130292</td>\n",
              "      <td>0.870736</td>\n",
              "      <td>-3.210528</td>\n",
              "      <td>0.062493</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>1.436060</td>\n",
              "      <td>1.465940</td>\n",
              "      <td>0.636203</td>\n",
              "      <td>0.282354</td>\n",
              "      <td>0.515587</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.116585</td>\n",
              "      <td>0.069915</td>\n",
              "      <td>0.068476</td>\n",
              "      <td>-0.783147</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>0.439463</td>\n",
              "      <td>-0.081827</td>\n",
              "      <td>-0.888236</td>\n",
              "      <td>-0.123005</td>\n",
              "      <td>0.582939</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.031541</td>\n",
              "      <td>0.297600</td>\n",
              "      <td>-3.210528</td>\n",
              "      <td>-0.583590</td>\n",
              "      <td>-0.721442</td>\n",
              "      <td>-0.307984</td>\n",
              "      <td>0.710932</td>\n",
              "      <td>1.051693</td>\n",
              "      <td>0.594169</td>\n",
              "      <td>-0.533994</td>\n",
              "      <td>A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.229043</td>\n",
              "      <td>0.807926</td>\n",
              "      <td>-0.052442</td>\n",
              "      <td>0.082634</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>0.148790</td>\n",
              "      <td>0.635431</td>\n",
              "      <td>0.051062</td>\n",
              "      <td>0.032902</td>\n",
              "      <td>-0.086652</td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   intercolumnar distance  upper margin  lower margin  exploitation  \\\n",
              "0                0.266074     -0.165620      0.320980      0.483299   \n",
              "1                0.130292      0.870736     -3.210528      0.062493   \n",
              "2               -0.116585      0.069915      0.068476     -0.783147   \n",
              "3                0.031541      0.297600     -3.210528     -0.583590   \n",
              "4                0.229043      0.807926     -0.052442      0.082634   \n",
              "\n",
              "   row number  modular ratio  interlinear spacing    weight  peak number  \\\n",
              "0    0.172340       0.273364             0.371178  0.929823     0.251173   \n",
              "1    0.261718       1.436060             1.465940  0.636203     0.282354   \n",
              "2    0.261718       0.439463            -0.081827 -0.888236    -0.123005   \n",
              "3   -0.721442      -0.307984             0.710932  1.051693     0.594169   \n",
              "4    0.261718       0.148790             0.635431  0.051062     0.032902   \n",
              "\n",
              "   m_ratio/ i_spacing class  \n",
              "0            0.159345     A  \n",
              "1            0.515587     A  \n",
              "2            0.582939     A  \n",
              "3           -0.533994     A  \n",
              "4           -0.086652     F  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "S5QDVitUsnGB",
        "colab_type": "code",
        "outputId": "a4b00b49-4a04-4afe-a881-91f855f6dd9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "train_data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intercolumnar distance</th>\n",
              "      <th>upper margin</th>\n",
              "      <th>lower margin</th>\n",
              "      <th>exploitation</th>\n",
              "      <th>row number</th>\n",
              "      <th>modular ratio</th>\n",
              "      <th>interlinear spacing</th>\n",
              "      <th>weight</th>\n",
              "      <th>peak number</th>\n",
              "      <th>m_ratio/ i_spacing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "      <td>10430.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.000852</td>\n",
              "      <td>0.033611</td>\n",
              "      <td>-0.000525</td>\n",
              "      <td>-0.002387</td>\n",
              "      <td>0.006370</td>\n",
              "      <td>0.013973</td>\n",
              "      <td>0.005605</td>\n",
              "      <td>0.010323</td>\n",
              "      <td>0.012914</td>\n",
              "      <td>0.000818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.991431</td>\n",
              "      <td>3.920868</td>\n",
              "      <td>1.120202</td>\n",
              "      <td>1.008527</td>\n",
              "      <td>0.992053</td>\n",
              "      <td>1.126245</td>\n",
              "      <td>1.313754</td>\n",
              "      <td>1.003507</td>\n",
              "      <td>1.087665</td>\n",
              "      <td>1.007094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.498799</td>\n",
              "      <td>-2.426761</td>\n",
              "      <td>-3.210528</td>\n",
              "      <td>-5.440122</td>\n",
              "      <td>-4.922215</td>\n",
              "      <td>-7.450257</td>\n",
              "      <td>-11.935457</td>\n",
              "      <td>-4.247781</td>\n",
              "      <td>-5.486218</td>\n",
              "      <td>-6.719324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.128929</td>\n",
              "      <td>-0.259834</td>\n",
              "      <td>0.064919</td>\n",
              "      <td>-0.528002</td>\n",
              "      <td>0.172340</td>\n",
              "      <td>-0.598658</td>\n",
              "      <td>-0.044076</td>\n",
              "      <td>-0.541992</td>\n",
              "      <td>-0.372457</td>\n",
              "      <td>-0.516097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.043885</td>\n",
              "      <td>-0.055704</td>\n",
              "      <td>0.217845</td>\n",
              "      <td>0.095763</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>-0.058835</td>\n",
              "      <td>0.220177</td>\n",
              "      <td>0.111803</td>\n",
              "      <td>0.064084</td>\n",
              "      <td>-0.034513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.204355</td>\n",
              "      <td>0.203385</td>\n",
              "      <td>0.352988</td>\n",
              "      <td>0.658210</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>0.564038</td>\n",
              "      <td>0.446679</td>\n",
              "      <td>0.654944</td>\n",
              "      <td>0.500624</td>\n",
              "      <td>0.530855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.819916</td>\n",
              "      <td>386.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>3.987152</td>\n",
              "      <td>1.066121</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>13.173081</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>4.671232</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       intercolumnar distance  upper margin  lower margin  exploitation  \\\n",
              "count            10430.000000  10430.000000  10430.000000  10430.000000   \n",
              "mean                 0.000852      0.033611     -0.000525     -0.002387   \n",
              "std                  0.991431      3.920868      1.120202      1.008527   \n",
              "min                 -3.498799     -2.426761     -3.210528     -5.440122   \n",
              "25%                 -0.128929     -0.259834      0.064919     -0.528002   \n",
              "50%                  0.043885     -0.055704      0.217845      0.095763   \n",
              "75%                  0.204355      0.203385      0.352988      0.658210   \n",
              "max                 11.819916    386.000000     50.000000      3.987152   \n",
              "\n",
              "         row number  modular ratio  interlinear spacing        weight  \\\n",
              "count  10430.000000   10430.000000         10430.000000  10430.000000   \n",
              "mean       0.006370       0.013973             0.005605      0.010323   \n",
              "std        0.992053       1.126245             1.313754      1.003507   \n",
              "min       -4.922215      -7.450257           -11.935457     -4.247781   \n",
              "25%        0.172340      -0.598658            -0.044076     -0.541992   \n",
              "50%        0.261718      -0.058835             0.220177      0.111803   \n",
              "75%        0.261718       0.564038             0.446679      0.654944   \n",
              "max        1.066121      53.000000            83.000000     13.173081   \n",
              "\n",
              "        peak number  m_ratio/ i_spacing  \n",
              "count  10430.000000        10430.000000  \n",
              "mean       0.012914            0.000818  \n",
              "std        1.087665            1.007094  \n",
              "min       -5.486218           -6.719324  \n",
              "25%       -0.372457           -0.516097  \n",
              "50%        0.064084           -0.034513  \n",
              "75%        0.500624            0.530855  \n",
              "max       44.000000            4.671232  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "XM4UEP70s7i4",
        "colab_type": "code",
        "outputId": "5dee9451-2eb8-4ade-80fe-41df8b988aac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "test_data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intercolumnar distance</th>\n",
              "      <th>upper margin</th>\n",
              "      <th>lower margin</th>\n",
              "      <th>exploitation</th>\n",
              "      <th>row number</th>\n",
              "      <th>modular ratio</th>\n",
              "      <th>interlinear spacing</th>\n",
              "      <th>weight</th>\n",
              "      <th>peak number</th>\n",
              "      <th>m_ratio/ i_spacing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "      <td>10437.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-0.000852</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.005181</td>\n",
              "      <td>0.002616</td>\n",
              "      <td>-0.006365</td>\n",
              "      <td>-0.008886</td>\n",
              "      <td>0.002350</td>\n",
              "      <td>-0.010259</td>\n",
              "      <td>-0.008691</td>\n",
              "      <td>-0.000678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.008551</td>\n",
              "      <td>0.955257</td>\n",
              "      <td>0.992430</td>\n",
              "      <td>0.991443</td>\n",
              "      <td>1.007876</td>\n",
              "      <td>1.000360</td>\n",
              "      <td>0.966827</td>\n",
              "      <td>0.996431</td>\n",
              "      <td>1.001240</td>\n",
              "      <td>0.992928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.498799</td>\n",
              "      <td>-2.426761</td>\n",
              "      <td>-3.210528</td>\n",
              "      <td>-5.440122</td>\n",
              "      <td>-4.922215</td>\n",
              "      <td>-7.450257</td>\n",
              "      <td>-11.935457</td>\n",
              "      <td>-4.090167</td>\n",
              "      <td>-4.737863</td>\n",
              "      <td>-6.719324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.128929</td>\n",
              "      <td>-0.259834</td>\n",
              "      <td>0.064919</td>\n",
              "      <td>-0.526838</td>\n",
              "      <td>0.172340</td>\n",
              "      <td>-0.598658</td>\n",
              "      <td>-0.044076</td>\n",
              "      <td>-0.547709</td>\n",
              "      <td>-0.372457</td>\n",
              "      <td>-0.514199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.056229</td>\n",
              "      <td>-0.063555</td>\n",
              "      <td>0.217845</td>\n",
              "      <td>0.087408</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>-0.058835</td>\n",
              "      <td>0.220177</td>\n",
              "      <td>0.103541</td>\n",
              "      <td>0.064084</td>\n",
              "      <td>-0.020397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.204355</td>\n",
              "      <td>0.203385</td>\n",
              "      <td>0.356544</td>\n",
              "      <td>0.627208</td>\n",
              "      <td>0.261718</td>\n",
              "      <td>0.564038</td>\n",
              "      <td>0.446679</td>\n",
              "      <td>0.639426</td>\n",
              "      <td>0.500624</td>\n",
              "      <td>0.526304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>11.819916</td>\n",
              "      <td>19.470188</td>\n",
              "      <td>7.458681</td>\n",
              "      <td>3.987152</td>\n",
              "      <td>1.066121</td>\n",
              "      <td>12.315569</td>\n",
              "      <td>4.901228</td>\n",
              "      <td>4.580832</td>\n",
              "      <td>3.213413</td>\n",
              "      <td>11.911338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       intercolumnar distance  upper margin  lower margin  exploitation  \\\n",
              "count            10437.000000  10437.000000  10437.000000  10437.000000   \n",
              "mean                -0.000852      0.003396      0.005181      0.002616   \n",
              "std                  1.008551      0.955257      0.992430      0.991443   \n",
              "min                 -3.498799     -2.426761     -3.210528     -5.440122   \n",
              "25%                 -0.128929     -0.259834      0.064919     -0.526838   \n",
              "50%                  0.056229     -0.063555      0.217845      0.087408   \n",
              "75%                  0.204355      0.203385      0.356544      0.627208   \n",
              "max                 11.819916     19.470188      7.458681      3.987152   \n",
              "\n",
              "         row number  modular ratio  interlinear spacing        weight  \\\n",
              "count  10437.000000   10437.000000         10437.000000  10437.000000   \n",
              "mean      -0.006365      -0.008886             0.002350     -0.010259   \n",
              "std        1.007876       1.000360             0.966827      0.996431   \n",
              "min       -4.922215      -7.450257           -11.935457     -4.090167   \n",
              "25%        0.172340      -0.598658            -0.044076     -0.547709   \n",
              "50%        0.261718      -0.058835             0.220177      0.103541   \n",
              "75%        0.261718       0.564038             0.446679      0.639426   \n",
              "max        1.066121      12.315569             4.901228      4.580832   \n",
              "\n",
              "        peak number  m_ratio/ i_spacing  \n",
              "count  10437.000000        10437.000000  \n",
              "mean      -0.008691           -0.000678  \n",
              "std        1.001240            0.992928  \n",
              "min       -4.737863           -6.719324  \n",
              "25%       -0.372457           -0.514199  \n",
              "50%        0.064084           -0.020397  \n",
              "75%        0.500624            0.526304  \n",
              "max        3.213413           11.911338  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "H4YEhS1NtDJ2",
        "colab_type": "code",
        "outputId": "23ba50e6-cc4b-494d-80f8-2699d1d02ee3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "# Test if our datasets are close to each other or not\n",
        "train_data.describe() - test_data.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>intercolumnar distance</th>\n",
              "      <th>upper margin</th>\n",
              "      <th>lower margin</th>\n",
              "      <th>exploitation</th>\n",
              "      <th>row number</th>\n",
              "      <th>modular ratio</th>\n",
              "      <th>interlinear spacing</th>\n",
              "      <th>weight</th>\n",
              "      <th>peak number</th>\n",
              "      <th>m_ratio/ i_spacing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "      <td>-7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.030215</td>\n",
              "      <td>-0.005707</td>\n",
              "      <td>-0.005003</td>\n",
              "      <td>0.012735</td>\n",
              "      <td>0.022859</td>\n",
              "      <td>0.003255</td>\n",
              "      <td>0.020582</td>\n",
              "      <td>0.021605</td>\n",
              "      <td>0.001496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>-0.017120</td>\n",
              "      <td>2.965611</td>\n",
              "      <td>0.127772</td>\n",
              "      <td>0.017084</td>\n",
              "      <td>-0.015822</td>\n",
              "      <td>0.125885</td>\n",
              "      <td>0.346928</td>\n",
              "      <td>0.007076</td>\n",
              "      <td>0.086426</td>\n",
              "      <td>0.014166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.157614</td>\n",
              "      <td>-0.748355</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.001164</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005717</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.001898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.012344</td>\n",
              "      <td>0.007851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008355</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008262</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.014116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.003556</td>\n",
              "      <td>0.031002</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015518</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>366.529812</td>\n",
              "      <td>42.541319</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.684431</td>\n",
              "      <td>78.098772</td>\n",
              "      <td>8.592249</td>\n",
              "      <td>40.786587</td>\n",
              "      <td>-7.240106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       intercolumnar distance  upper margin  lower margin  exploitation  \\\n",
              "count               -7.000000     -7.000000     -7.000000     -7.000000   \n",
              "mean                 0.001704      0.030215     -0.005707     -0.005003   \n",
              "std                 -0.017120      2.965611      0.127772      0.017084   \n",
              "min                  0.000000      0.000000      0.000000      0.000000   \n",
              "25%                  0.000000      0.000000      0.000000     -0.001164   \n",
              "50%                 -0.012344      0.007851      0.000000      0.008355   \n",
              "75%                  0.000000      0.000000     -0.003556      0.031002   \n",
              "max                  0.000000    366.529812     42.541319      0.000000   \n",
              "\n",
              "       row number  modular ratio  interlinear spacing    weight  peak number  \\\n",
              "count   -7.000000      -7.000000            -7.000000 -7.000000    -7.000000   \n",
              "mean     0.012735       0.022859             0.003255  0.020582     0.021605   \n",
              "std     -0.015822       0.125885             0.346928  0.007076     0.086426   \n",
              "min      0.000000       0.000000             0.000000 -0.157614    -0.748355   \n",
              "25%      0.000000       0.000000             0.000000  0.005717     0.000000   \n",
              "50%      0.000000       0.000000             0.000000  0.008262     0.000000   \n",
              "75%      0.000000       0.000000             0.000000  0.015518     0.000000   \n",
              "max      0.000000      40.684431            78.098772  8.592249    40.786587   \n",
              "\n",
              "       m_ratio/ i_spacing  \n",
              "count           -7.000000  \n",
              "mean             0.001496  \n",
              "std              0.014166  \n",
              "min              0.000000  \n",
              "25%             -0.001898  \n",
              "50%             -0.014116  \n",
              "75%              0.004551  \n",
              "max             -7.240106  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "JTVeFl5ttZuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems that both our datasets are close to each other: training it on the train data should yield great results over the test set\n",
        "\n",
        "The question we need to answer is: if they exist, which of the columns can help us tell the difference between two authors?"
      ]
    },
    {
      "metadata": {
        "id": "cI-aDVwcOcQq",
        "colab_type": "code",
        "outputId": "04aa5cf7-479f-425e-8793-4769b1aaedee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "cell_type": "code",
      "source": [
        "# Print out the correlations between our variables\n",
        "corr = train_data.corr(method=\"pearson\")\n",
        "sns.heatmap(corr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f9fa659f470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh4AAAGkCAYAAAB+eizEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAyNH7Qrms7NQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "KjsFJ3p-6ATs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It seems that our variables are not correlated, we are going to keep them all for now"
      ]
    },
    {
      "metadata": {
        "id": "Xp9uvW8ZYHVO",
        "colab_type": "code",
        "outputId": "09ca7183-5c10-4dd0-a864-056c2d2fb3bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Separating data in features - targets (x and y)\n",
        "x = train_data.drop(columns=[\"class\"])\n",
        "y = train_data[\"class\"]\n",
        "\n",
        "# Running the feature ranking function on our features\n",
        "feature_ranking = SelectKBest(k=5)\n",
        "fmt = '%-8s%-20s%s'\n",
        "feature_ranking.fit(x, y)\n",
        "for i, (score, feature) in enumerate(zip(feature_ranking.scores_, x.columns)):\n",
        "    print(fmt % (i, score, feature))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       69.364154161064     intercolumnar distance\n",
            "1       6.4648031925083895  upper margin\n",
            "2       12.546385854687312  lower margin\n",
            "3       143.4966501011398   exploitation\n",
            "4       81.99985336305242   row number\n",
            "5       91.02632927858829   modular ratio\n",
            "6       95.62292149631013   interlinear spacing\n",
            "7       30.03858906826128   weight\n",
            "8       214.79469083166097  peak number\n",
            "9       63.520889416727485  m_ratio/ i_spacing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-SVN_m9iisr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thanks to this bit of code we can see that the five most intereting features are the peak number, the exploitation, the interlinear spacing, the modular ratio and the row number"
      ]
    },
    {
      "metadata": {
        "id": "pw0K1bGAo1WO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating a bunch of data with the 6 best columns only\n",
        "x_dropped = train_data[[\"peak number\", \"exploitation\", \"interlinear spacing\", \"modular ratio\", \"row number\", \"intercolumnar distance\"]]\n",
        "x_test = test_data.drop(columns=[\"class\"])\n",
        "x_test_dropped = test_data[[\"peak number\", \"exploitation\", \"interlinear spacing\", \"modular ratio\", \"row number\", \"intercolumnar distance\"]]\n",
        "y_test = test_data[\"class\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pKpYXCyDiiAt",
        "colab_type": "code",
        "outputId": "2d302505-1d56-420f-d29c-5621284e20d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's first try out a basic logistic regression on the datas with dropped columns\n",
        "lr_dropped = LogisticRegression()\n",
        "lr_dropped.fit(x_dropped, y)\n",
        "lr_dropped.score(x_dropped, y), lr_dropped.score(x_test_dropped, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.511601150527325, 0.5075213183865095)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "GNc4_J2TSjdy",
        "colab_type": "code",
        "outputId": "f347752a-699c-46b6-b665-f9732aca948a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Now let's compare it to a model that hasn't columns dropped\n",
        "lr = LogisticRegression()\n",
        "lr.fit(x, y)\n",
        "lr.score(x, y), lr.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
            "  \"this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5338446788111217, 0.5307080578710357)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "rsgKoPnBTuU0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trees\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "Now that we tested the regression and that we did not get good results, let's try another type of algorithms: the decision trees. It seems that our training set is more precise when given all the features from the initial dataset. Let's try both entire and column-dropped datasets.\n",
        "\n",
        "A good description of the library and the algorithm itself can be found [here](https://scikit-learn.org/stable/modules/tree.html#tree).\n",
        "\n",
        "Let's try and run this algorithm with few different parameters."
      ]
    },
    {
      "metadata": {
        "id": "Tnznx785Ts3M",
        "colab_type": "code",
        "outputId": "5d0a2d60-248e-4fde-f8c5-2cf1a1b11932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "cell_type": "code",
      "source": [
        "depths = [5, 10, 15, 20]\n",
        "min_samples_leafs = [5, 10, 15, 20]\n",
        "\n",
        "\n",
        "cols = [\"depth\", \"min_samples_leaf\", \"accuracy_dropped\", \"accuracy_dropped_test\", \"accuracy\", \"accuracy_test\"]\n",
        "results = []\n",
        "\n",
        "for depth in depths:\n",
        "  for min_samples_leaf in min_samples_leafs:\n",
        "    dtree = DecisionTreeClassifier(max_depth=depth, random_state=101, max_features=None, min_samples_leaf=min_samples_leaf)\n",
        "    dtree.fit(x, y)\n",
        "    dtree_dropped = DecisionTreeClassifier(max_depth=depth, random_state=101, max_features=None, min_samples_leaf=min_samples_leaf)\n",
        "    dtree_dropped.fit(x_dropped, y)\n",
        "    \n",
        "    results.append([depth, min_samples_leaf, \n",
        "                    dtree_dropped.score(x_dropped, y), dtree_dropped.score(x_test_dropped, y_test),\n",
        "                    dtree.score(x, y), dtree.score(x_test, y_test)])\n",
        "    \n",
        "results_df = pd.DataFrame(results, columns=cols)\n",
        "results_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>depth</th>\n",
              "      <th>min_samples_leaf</th>\n",
              "      <th>accuracy_dropped</th>\n",
              "      <th>accuracy_dropped_test</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>accuracy_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.589645</td>\n",
              "      <td>0.586567</td>\n",
              "      <td>0.613615</td>\n",
              "      <td>0.603718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.589645</td>\n",
              "      <td>0.586567</td>\n",
              "      <td>0.613615</td>\n",
              "      <td>0.603718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>0.589645</td>\n",
              "      <td>0.586567</td>\n",
              "      <td>0.613615</td>\n",
              "      <td>0.603718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.589645</td>\n",
              "      <td>0.586567</td>\n",
              "      <td>0.613615</td>\n",
              "      <td>0.604388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.742282</td>\n",
              "      <td>0.719076</td>\n",
              "      <td>0.835666</td>\n",
              "      <td>0.808566</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0.733078</td>\n",
              "      <td>0.711507</td>\n",
              "      <td>0.829914</td>\n",
              "      <td>0.804062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>0.722339</td>\n",
              "      <td>0.699722</td>\n",
              "      <td>0.819080</td>\n",
              "      <td>0.792661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>0.713327</td>\n",
              "      <td>0.691578</td>\n",
              "      <td>0.806232</td>\n",
              "      <td>0.783079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>0.902876</td>\n",
              "      <td>0.862892</td>\n",
              "      <td>0.965580</td>\n",
              "      <td>0.938967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>0.857047</td>\n",
              "      <td>0.823896</td>\n",
              "      <td>0.938351</td>\n",
              "      <td>0.912714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>0.818504</td>\n",
              "      <td>0.783367</td>\n",
              "      <td>0.894535</td>\n",
              "      <td>0.864233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15</td>\n",
              "      <td>20</td>\n",
              "      <td>0.788015</td>\n",
              "      <td>0.756348</td>\n",
              "      <td>0.862895</td>\n",
              "      <td>0.832998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>0.938159</td>\n",
              "      <td>0.893839</td>\n",
              "      <td>0.975360</td>\n",
              "      <td>0.950561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>0.875551</td>\n",
              "      <td>0.838459</td>\n",
              "      <td>0.940940</td>\n",
              "      <td>0.916068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>0.825887</td>\n",
              "      <td>0.789307</td>\n",
              "      <td>0.895206</td>\n",
              "      <td>0.864425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>0.789262</td>\n",
              "      <td>0.757114</td>\n",
              "      <td>0.862895</td>\n",
              "      <td>0.832998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    depth  min_samples_leaf  accuracy_dropped  accuracy_dropped_test  \\\n",
              "0       5                 5          0.589645               0.586567   \n",
              "1       5                10          0.589645               0.586567   \n",
              "2       5                15          0.589645               0.586567   \n",
              "3       5                20          0.589645               0.586567   \n",
              "4      10                 5          0.742282               0.719076   \n",
              "5      10                10          0.733078               0.711507   \n",
              "6      10                15          0.722339               0.699722   \n",
              "7      10                20          0.713327               0.691578   \n",
              "8      15                 5          0.902876               0.862892   \n",
              "9      15                10          0.857047               0.823896   \n",
              "10     15                15          0.818504               0.783367   \n",
              "11     15                20          0.788015               0.756348   \n",
              "12     20                 5          0.938159               0.893839   \n",
              "13     20                10          0.875551               0.838459   \n",
              "14     20                15          0.825887               0.789307   \n",
              "15     20                20          0.789262               0.757114   \n",
              "\n",
              "    accuracy  accuracy_test  \n",
              "0   0.613615       0.603718  \n",
              "1   0.613615       0.603718  \n",
              "2   0.613615       0.603718  \n",
              "3   0.613615       0.604388  \n",
              "4   0.835666       0.808566  \n",
              "5   0.829914       0.804062  \n",
              "6   0.819080       0.792661  \n",
              "7   0.806232       0.783079  \n",
              "8   0.965580       0.938967  \n",
              "9   0.938351       0.912714  \n",
              "10  0.894535       0.864233  \n",
              "11  0.862895       0.832998  \n",
              "12  0.975360       0.950561  \n",
              "13  0.940940       0.916068  \n",
              "14  0.895206       0.864425  \n",
              "15  0.862895       0.832998  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "Wagh_KRSbeb2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "As we can see above, we yield better results when using all of the features. The \"min_samples_leaf\" parameter is the minimum number of examples needed for the tree to be forked into two leafs. The lower it is, the better accuracy we get.\n",
        "\n",
        "We need to be careful to overfitting when using a low number for this parameter. Indeed, when only 5 examples are needed to fork a branch then it is easier for the model to identify specific cases in our dataset, thus overfitting. \n",
        "\n",
        "In fact, we can tell that the better model for this dataset is either the 20-5 or 20-10 as we get close results on the test and train set\n",
        "\n",
        "### Random Forest\n",
        "\n",
        "We got pretty good results with the decision tree. Now let's try another type of tree to see if we can get better results !\n",
        "\n",
        "A good description of the library and the algorithm itself can be found [here](https://scikit-learn.org/stable/modules/ensemble.html#forest).\n",
        "\n",
        "Let's use the same approach we previously used for the decision tree."
      ]
    },
    {
      "metadata": {
        "id": "xE4XPNXuguEM",
        "colab_type": "code",
        "outputId": "29ca1e7c-2c6b-4ba8-f290-510a01ed7187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "cell_type": "code",
      "source": [
        "n_estimators = [5, 10, 15, 20]\n",
        "min_samples_leafs = [5, 10, 15, 20]\n",
        "\n",
        "\n",
        "cols = [\"n_estimator\", \"min_samples_leaf\", \"accuracy_dropped\", \"accuracy_dropped_test\", \"accuracy\", \"accuracy_test\"]\n",
        "results_rfm = []\n",
        "\n",
        "for n_estimator in n_estimators:\n",
        "  for min_samples_leaf in min_samples_leafs:\n",
        "    rfm = RandomForestClassifier(n_estimators=n_estimator, n_jobs=-1, random_state=101, max_features = None, min_samples_leaf=min_samples_leaf)\n",
        "    rfm.fit(x, y)\n",
        "    rfm_dropped = RandomForestClassifier(n_estimators=n_estimator, n_jobs=-1, random_state=101, max_features = None, min_samples_leaf=min_samples_leaf)\n",
        "    rfm_dropped.fit(x_dropped, y)\n",
        "    \n",
        "    results_rfm.append([n_estimator, min_samples_leaf, \n",
        "                    rfm_dropped.score(x_dropped, y), rfm_dropped.score(x_test_dropped, y_test),\n",
        "                    rfm.score(x, y), rfm.score(x_test, y_test)])\n",
        "    \n",
        "results_rfm_df = pd.DataFrame(results_rfm, columns=cols)\n",
        "results_rfm_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>n_estimator</th>\n",
              "      <th>min_samples_leaf</th>\n",
              "      <th>accuracy_dropped</th>\n",
              "      <th>accuracy_dropped_test</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>accuracy_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.937967</td>\n",
              "      <td>0.881575</td>\n",
              "      <td>0.980825</td>\n",
              "      <td>0.962250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>0.874209</td>\n",
              "      <td>0.825046</td>\n",
              "      <td>0.948706</td>\n",
              "      <td>0.921912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>0.830681</td>\n",
              "      <td>0.791607</td>\n",
              "      <td>0.913902</td>\n",
              "      <td>0.885024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.798658</td>\n",
              "      <td>0.764683</td>\n",
              "      <td>0.887440</td>\n",
              "      <td>0.860975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0.955609</td>\n",
              "      <td>0.904858</td>\n",
              "      <td>0.988686</td>\n",
              "      <td>0.974801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0.898754</td>\n",
              "      <td>0.853406</td>\n",
              "      <td>0.964909</td>\n",
              "      <td>0.946632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>10</td>\n",
              "      <td>15</td>\n",
              "      <td>0.845829</td>\n",
              "      <td>0.810099</td>\n",
              "      <td>0.936050</td>\n",
              "      <td>0.910990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>0.804698</td>\n",
              "      <td>0.773115</td>\n",
              "      <td>0.908341</td>\n",
              "      <td>0.881384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>15</td>\n",
              "      <td>5</td>\n",
              "      <td>0.959348</td>\n",
              "      <td>0.906870</td>\n",
              "      <td>0.988591</td>\n",
              "      <td>0.974897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>15</td>\n",
              "      <td>10</td>\n",
              "      <td>0.900863</td>\n",
              "      <td>0.854843</td>\n",
              "      <td>0.967018</td>\n",
              "      <td>0.947207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>0.847651</td>\n",
              "      <td>0.809811</td>\n",
              "      <td>0.937105</td>\n",
              "      <td>0.912044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>15</td>\n",
              "      <td>20</td>\n",
              "      <td>0.806328</td>\n",
              "      <td>0.774935</td>\n",
              "      <td>0.906999</td>\n",
              "      <td>0.879467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>0.961170</td>\n",
              "      <td>0.908020</td>\n",
              "      <td>0.989070</td>\n",
              "      <td>0.975951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>20</td>\n",
              "      <td>10</td>\n",
              "      <td>0.900479</td>\n",
              "      <td>0.855227</td>\n",
              "      <td>0.967498</td>\n",
              "      <td>0.944716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>20</td>\n",
              "      <td>15</td>\n",
              "      <td>0.848610</td>\n",
              "      <td>0.811153</td>\n",
              "      <td>0.938926</td>\n",
              "      <td>0.915110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>0.808725</td>\n",
              "      <td>0.775319</td>\n",
              "      <td>0.904314</td>\n",
              "      <td>0.877743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    n_estimator  min_samples_leaf  accuracy_dropped  accuracy_dropped_test  \\\n",
              "0             5                 5          0.937967               0.881575   \n",
              "1             5                10          0.874209               0.825046   \n",
              "2             5                15          0.830681               0.791607   \n",
              "3             5                20          0.798658               0.764683   \n",
              "4            10                 5          0.955609               0.904858   \n",
              "5            10                10          0.898754               0.853406   \n",
              "6            10                15          0.845829               0.810099   \n",
              "7            10                20          0.804698               0.773115   \n",
              "8            15                 5          0.959348               0.906870   \n",
              "9            15                10          0.900863               0.854843   \n",
              "10           15                15          0.847651               0.809811   \n",
              "11           15                20          0.806328               0.774935   \n",
              "12           20                 5          0.961170               0.908020   \n",
              "13           20                10          0.900479               0.855227   \n",
              "14           20                15          0.848610               0.811153   \n",
              "15           20                20          0.808725               0.775319   \n",
              "\n",
              "    accuracy  accuracy_test  \n",
              "0   0.980825       0.962250  \n",
              "1   0.948706       0.921912  \n",
              "2   0.913902       0.885024  \n",
              "3   0.887440       0.860975  \n",
              "4   0.988686       0.974801  \n",
              "5   0.964909       0.946632  \n",
              "6   0.936050       0.910990  \n",
              "7   0.908341       0.881384  \n",
              "8   0.988591       0.974897  \n",
              "9   0.967018       0.947207  \n",
              "10  0.937105       0.912044  \n",
              "11  0.906999       0.879467  \n",
              "12  0.989070       0.975951  \n",
              "13  0.967498       0.944716  \n",
              "14  0.938926       0.915110  \n",
              "15  0.904314       0.877743  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "hj6SOEJ-isak",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Results\n",
        "\n",
        "As seen before, we yield better results when using all the features that we have. The problem of overfitting also appears for this algorithm but when using a small number of features we got great results while reducing overfitting.\n",
        "\n",
        "We can say that the safest model to use to prevent overfitting is the 5-10.\n",
        "\n",
        "As we found that both two algorithms yield great results, we can tell that our analysis is satisfying. We could have tried several other algorithms, such as Deep Learning ones, but they need a high computing power, and it seems that our actual results are hard to improve."
      ]
    }
  ]
}